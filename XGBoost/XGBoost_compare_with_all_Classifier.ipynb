{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preprocession Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1\n",
      " 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1\n",
      " 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1\n",
      " 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 1\n",
      " 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1\n",
      " 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0\n",
      " 1 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0\n",
      " 0 1 1 0 0 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0\n",
      " 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1\n",
      " 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0\n",
      " 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1]\n",
      "[[ 142932       7       6 ...       9      10       2]\n",
      " [1120559       8       3 ...       8       9       8]\n",
      " [1254538       8      10 ...      10      10       1]\n",
      " ...\n",
      " [1214092       1       1 ...       1       1       1]\n",
      " [1303489       3       1 ...       2       1       1]\n",
      " [ 378275      10       9 ...       7       7       1]]\n",
      "[1 1 1 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1\n",
      " 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1\n",
      " 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0\n",
      " 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1\n",
      " 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0\n",
      " 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0\n",
      " 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1]\n",
      "[[1173347       1       1 ...       1       1       1]\n",
      " [1156017       3       1 ...       2       1       1]\n",
      " [ 706426       5       5 ...       4       3       1]\n",
      " ...\n",
      " [ 764974       5       1 ...       3       1       2]\n",
      " [1137156       2       2 ...       7       1       1]\n",
      " [1160476       2       1 ...       3       1       1]]\n",
      "[0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1\n",
      " 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0\n",
      " 1 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('./Datasets/Data.csv')\n",
    "# print(dataset)\n",
    "\n",
    "X = dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,-1].values\n",
    "\n",
    "\n",
    "# # Taking care of Missing values\n",
    "# from sklearn.impute import SimpleImputer \n",
    "# imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# imputer.fit(X[:,:-1])\n",
    "# X[:,:-1] =  imputer.transform(X[:,:-1])\n",
    "# imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "# imputer.fit(X[:,-2:-1])\n",
    "# X[:,-2:-1] =  imputer.transform(X[:,-2:-1])\n",
    "# # print(X)\n",
    "\n",
    "# # Encoding categorial Data [One Hot Encoding]\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# ct = ColumnTransformer(transformers=[('encode',OneHotEncoder(),[-1])], remainder='passthrough')\n",
    "# X = np.array(ct.fit_transform(X))\n",
    "# # print(X)\n",
    "\n",
    "\n",
    "# LabelEncoder for y\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(y)\n",
    "\n",
    "# Splitting dataset into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "# fit Score of  training data\n",
    "sc.fit(X_train)\n",
    "# transform training data using the scaler\n",
    "X_train = sc.transform(X_train)\n",
    "# transform test data using the same scaler\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Classification Models Score of  Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Logisitic Regression Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_LR = LogisticRegression(random_state = 0)\n",
    "classifier_LR.fit(X_train,y_train)\n",
    "y_pred_LR = classifier_LR.predict(X_test)\n",
    "\n",
    "# Using K-Nearest Neighbor Classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_KNN = KNeighborsClassifier(n_neighbors = 5, metric = \"minkowski\", p = 2)\n",
    "classifier_KNN.fit(X_train,y_train)\n",
    "y_pred_KNN = classifier_KNN.predict(X_test)\n",
    "\n",
    "# Using Support Vector Classification\n",
    "from sklearn.svm import SVC\n",
    "classifier_SVC = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier_SVC.fit(X_train,y_train)\n",
    "y_pred_SVC = classifier_SVC.predict(X_test)\n",
    "\n",
    "# Using Kernel SVM Classification\n",
    "from sklearn.svm import SVC\n",
    "classifier_KSVM = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier_KSVM.fit(X_train,y_train)\n",
    "y_pred_KSVM = classifier_KSVM.predict(X_test)\n",
    "\n",
    "\n",
    "# Using Naive Bayes Classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier_NB = GaussianNB()\n",
    "classifier_NB.fit(X_train,y_train)\n",
    "y_pred_NB = classifier_NB.predict(X_test)\n",
    "\n",
    "\n",
    "# Using Decision Tree Classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier_DT = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier_DT.fit(X_train,y_train)\n",
    "y_pred_DT = classifier_DT.predict(X_test)\n",
    "\n",
    "\n",
    "# Using Random Forest Classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier_RF = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier_RF.fit(X_train,y_train)\n",
    "y_pred_RF = classifier_RF.predict(X_test)\n",
    "\n",
    "# Using XGBoost Classification\n",
    "from xgboost import XGBClassifier\n",
    "classifier_XGBoost = XGBClassifier()\n",
    "classifier_XGBoost.fit(X_train,y_train)\n",
    "y_pred_XGBoost = classifier_XGBoost.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix and comparing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification : 0.9473684210526315\n",
      "K-Nearest Neighbor Classification : 0.9473684210526315\n",
      "Support Vector Classification : 0.9415204678362573\n",
      "Kernel SVM Classification : 0.9532163742690059\n",
      "Naive Bayes Classification : 0.9415204678362573\n",
      "Decision Tree Classification : 0.9590643274853801\n",
      "Random Forest Classification : 0.9590643274853801\n",
      "XGBoost Classification : 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "Score =[]\n",
    "\n",
    "# Using Logisitic Regression Classification\n",
    "cm = confusion_matrix(y_test,y_pred_LR)\n",
    "acs = accuracy_score(y_test,y_pred_LR)\n",
    "Score.append([\"Logistic Regression Classification\", cm, acs])\n",
    "\n",
    "# Using K-Nearest Neighbor Classification\n",
    "cm = confusion_matrix(y_test,y_pred_KNN)\n",
    "acs = accuracy_score(y_test,y_pred_KNN)\n",
    "Score.append([\"K-Nearest Neighbor Classification\", cm, acs])\n",
    "\n",
    "# Using Support Vector Classification\n",
    "cm = confusion_matrix(y_test,y_pred_SVC)\n",
    "acs = accuracy_score(y_test,y_pred_SVC)\n",
    "Score.append([\"Support Vector Classification\", cm, acs])\n",
    "\n",
    "# Using Kernel SVM Classification\n",
    "cm = confusion_matrix(y_test,y_pred_KSVM)\n",
    "acs = accuracy_score(y_test,y_pred_KSVM)\n",
    "Score.append([\"Kernel SVM Classification\", cm, acs])\n",
    "\n",
    "\n",
    "# Using Naive Bayes Classification\n",
    "cm = confusion_matrix(y_test,y_pred_NB)\n",
    "acs = accuracy_score(y_test,y_pred_NB)\n",
    "Score.append([\"Naive Bayes Classification\", cm, acs])\n",
    "\n",
    "\n",
    "# Using Decision Tree Classification\n",
    "cm = confusion_matrix(y_test,y_pred_DT)\n",
    "acs = accuracy_score(y_test,y_pred_DT)\n",
    "Score.append([\"Decision Tree Classification\", cm, acs])\n",
    "\n",
    "\n",
    "# Using Random Forest Classification\n",
    "cm = confusion_matrix(y_test,y_pred_RF)\n",
    "acs = accuracy_score(y_test,y_pred_RF)\n",
    "Score.append([\"Random Forest Classification\", cm, acs])\n",
    "\n",
    "\n",
    "# Using XGBoost Classification\n",
    "cm = confusion_matrix(y_test,y_pred_XGBoost)\n",
    "acs = accuracy_score(y_test,y_pred_XGBoost)\n",
    "Score.append([\"XGBoost Classification\", cm, acs])\n",
    "\n",
    "\n",
    "\n",
    "for score in Score:\n",
    "    print(score[0],\":\",score[2])\n",
    "    # print(\"Confusion matrix\")\n",
    "    # print(score[1])\n",
    "# Confusion Matrix\n",
    "# [\n",
    "#     [Correct-0,   Incorrect-1]\n",
    "#     [Incorrect-0, Correct-0]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying K-Fold Cross Validation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Logistic Regression Classification\n",
      "Accuracy : 96.87 %\n",
      "Standard Deviation : 1.57 %\n",
      "For K-Nearest Neighbor Classification\n",
      "Accuracy : 97.46 %\n",
      "Standard Deviation : 2.14 %\n",
      "For Support Vector Classification\n",
      "Accuracy : 97.27 %\n",
      "Standard Deviation : 2.00 %\n",
      "For Kernel SVM Classification\n",
      "Accuracy : 97.07 %\n",
      "Standard Deviation : 2.18 %\n",
      "For Naive Bayes Classification\n",
      "Accuracy : 96.88 %\n",
      "Standard Deviation : 2.17 %\n",
      "For Decision Tree Classification\n",
      "Accuracy : 94.53 %\n",
      "Standard Deviation : 1.91 %\n",
      "For Random Forest Classification\n",
      "Accuracy : 94.53 %\n",
      "Standard Deviation : 1.91 %\n",
      "For XGBoost Classification\n",
      "Accuracy : 96.29 %\n",
      "Standard Deviation : 2.84 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Using Logisitic Regression Classification\n",
    "accuracies = cross_val_score(\n",
    "        estimator=classifier_LR,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=10\n",
    "    )\n",
    "\n",
    "print(\"For Logistic Regression Classification\")\n",
    "print(\"Accuracy : {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation : {:.2f} %\".format(accuracies.std()*100))\n",
    "\n",
    "\n",
    "# Using K-Nearest Neighbor Classification\n",
    "accuracies = cross_val_score(\n",
    "        estimator=classifier_KNN,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=10\n",
    "    )\n",
    "\n",
    "print(\"For K-Nearest Neighbor Classification\")\n",
    "print(\"Accuracy : {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation : {:.2f} %\".format(accuracies.std()*100))\n",
    "\n",
    "\n",
    "\n",
    "# Using Support Vector Classification\n",
    "accuracies = cross_val_score(\n",
    "        estimator=classifier_SVC,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=10\n",
    "    )\n",
    "\n",
    "print(\"For Support Vector Classification\")\n",
    "print(\"Accuracy : {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation : {:.2f} %\".format(accuracies.std()*100))\n",
    "\n",
    "\n",
    "# Using Kernel SVM Classification\n",
    "accuracies = cross_val_score(\n",
    "        estimator=classifier_KSVM,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=10\n",
    "    )\n",
    "\n",
    "print(\"For Kernel SVM Classification\")\n",
    "print(\"Accuracy : {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation : {:.2f} %\".format(accuracies.std()*100))\n",
    "\n",
    "\n",
    "# Using Naive Bayes Classification\n",
    "accuracies = cross_val_score(\n",
    "        estimator=classifier_NB,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=10\n",
    "    )\n",
    "\n",
    "print(\"For Naive Bayes Classification\")\n",
    "print(\"Accuracy : {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation : {:.2f} %\".format(accuracies.std()*100))\n",
    "\n",
    "\n",
    "\n",
    "# Using Decision Tree Classification\n",
    "accuracies = cross_val_score(\n",
    "        estimator=classifier_DT,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=10\n",
    "    )\n",
    "\n",
    "print(\"For Decision Tree Classification\")\n",
    "print(\"Accuracy : {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation : {:.2f} %\".format(accuracies.std()*100))\n",
    "\n",
    "\n",
    "# Using Random Forest Classification\n",
    "accuracies = cross_val_score(\n",
    "        estimator=classifier_RF,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=10\n",
    "    )\n",
    "\n",
    "print(\"For Random Forest Classification\")\n",
    "print(\"Accuracy : {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation : {:.2f} %\".format(accuracies.std()*100))\n",
    "\n",
    "\n",
    "\n",
    "# Using XGBoost Classification\n",
    "accuracies = cross_val_score(\n",
    "        estimator=classifier_XGBoost,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=10\n",
    "    )\n",
    "\n",
    "print(\"For XGBoost Classification\")\n",
    "print(\"Accuracy : {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation : {:.2f} %\".format(accuracies.std()*100))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('mlvenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "321334cedfa16b8be24125e5ab0e3623912f69d4b899407428e8bc07c2d3a5b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
