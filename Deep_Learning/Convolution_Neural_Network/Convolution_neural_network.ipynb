{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "In this we try to mimic the human brain [neural network], Using some layers\n",
    "- Input Layer:Input\n",
    "- Hidden Layer: Process\n",
    "- Output Layer: Output\n",
    "\n",
    "<img src=\"../Deep_learning.png\"/>\n",
    "\n",
    "## Convolution Neural Network [ CNN ]\n",
    "In the below images\n",
    "\n",
    "<hr/>\n",
    "\n",
    "- Image with two Faces \n",
    "\n",
    "<img src=\"./Man_with_two_faces.png\"/>\n",
    "\n",
    "    1. Man loooking to the right\n",
    "    2. Man looking at you\n",
    "\n",
    "<hr/>\n",
    "\n",
    "- Lady with two Faces \n",
    "\n",
    "<img src=\"./Lady_with_two_ages.png\"/>\n",
    "\n",
    "    1. Girl loooking away\n",
    "    2. Old lady looking down\n",
    "\n",
    "<hr/>\n",
    "\n",
    "- Duck or the Rabit \n",
    "\n",
    "<img src=\"./Duck_or_the_rabit.png\"/>\n",
    "\n",
    "    1. Duck\n",
    "    2. Rabit\n",
    "\n",
    "<hr/>\n",
    "\n",
    "- Image with Moving Face \n",
    "\n",
    "<img src=\"./Image_with_moving_face.png\"/>\n",
    "\n",
    "    1. Upper one is actual Face\n",
    "    2. Lower one is actual Face\n",
    "\n",
    "<hr/>\n",
    "\n",
    "The above images can looki like 1<sup>st</sup> or the 2<sup>nd</sup>, based on the features on which we are focusing.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "\n",
    "\n",
    "How CNN Works?\n",
    "\n",
    "<img src=\"./CNN_processing.png\"/>\n",
    "\n",
    "CNN Examples\n",
    "\n",
    "<img src=\"./CNN_example.png\"/>\n",
    "\n",
    "How a computer looks on an image?\n",
    "\n",
    "<img src=\"./Image_for_computer.png\"/>\n",
    "\n",
    "Steps of CNN\n",
    "\n",
    "1. Convolution: Detecting the features\n",
    "\n",
    "    Function for convolution\n",
    "    $$ (f*g) \\overset{\\underset{\\mathrm{def}}{}}{=} \\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau)d\\tau $$\n",
    "\n",
    "    In graphical view\n",
    "\n",
    "    <img src=\"./CNN_step_1.png\"/>\n",
    "\n",
    "    This can be understood with the below diagram\n",
    "\n",
    "    <img src=\"./CNN_step_1_explored.png\"/>\n",
    "\n",
    "    Some common filters\n",
    "\n",
    "    <img src=\"./Sharpen_filter.png\"/>\n",
    "\n",
    "    <img src=\"./Blur_filter.png\"/>\n",
    "\n",
    "    <img src=\"./Edge_enhance_filter.png\"/>\n",
    "\n",
    "    <img src=\"./Edge_detect_filter.png\"/>\n",
    "\n",
    "    <img src=\"./Emboss_filter.png\"/>\n",
    "\n",
    "    When we apply convolution, we loose some of the patterns of the image, features become linear, so to remove this linearity and imporove our feature/pattern, we apply the rectifier function, **ReLU Layer**\n",
    "\n",
    "    <img src=\"./ReLU_Layer.png\"/>\n",
    "\n",
    "    Example,\n",
    "\n",
    "    Original Image\n",
    "\n",
    "    <img src=\"./Original_image.png\"/>\n",
    "\n",
    "    Image after applying convolution\n",
    "\n",
    "    <img src=\"./Convolution_applied_image.png\"/>\n",
    "\n",
    "    Now after ReLU Layer\n",
    "\n",
    "    <img src=\"./ReLU_convolution_applied_image.png\"/>\n",
    "\n",
    "\n",
    "2. Max Pooling : Our object can have different rotation/position/texture in the image, for example\n",
    "\n",
    "    <img src=\"./Rotated_images.png\"/>\n",
    "\n",
    "    Or, below ones\n",
    "\n",
    "    <img src=\"./Different_images.png\"/>\n",
    "\n",
    "    \n",
    "    In Max Pooling, we take a NxN part of the matrix, N can be 2, 3, ... etc. and Now we select the max value of that portion, and we take from left_top to bottom_right, and get a max pooled data.\n",
    "\n",
    "    Applying Max Pooling to the data after Convolution\n",
    "\n",
    "    <img src=\"./Max_pooling_process.png\"/>\n",
    "\n",
    "    This will inhance the features, and hence enables to detect object in any rotation, position. Also, this reduces the size of data, hence improve processing.\n",
    "\n",
    "    There are some other poolings\n",
    "    1. Sum Pooling\n",
    "    2. Average Pooling, etc.\n",
    "\n",
    "    Below is the different layers of Number 4\n",
    "\n",
    "    <img src=\"./Detect_number_4.png\"/>\n",
    "\n",
    "\n",
    "3. Flattening : Means converting the matrix data into one column [ dimension ]\n",
    "\n",
    "    Below is the process of flattening\n",
    "\n",
    "    <img src=\"./Flattening_process.png\"/>\n",
    "\n",
    "    Getting Input Layer\n",
    "\n",
    "    <img src=\"./Convolution_Input_layer.png\"/>\n",
    "\n",
    "4. Full Connection: It is the step after flattening the data (getting the input layer)\n",
    "\n",
    "    <img src=\"./Full_connection_process.png\"/>\n",
    "\n",
    "    Detect Cat and Dog\n",
    "\n",
    "    <img src=\"./Detect_cat_dog.png\"/>\n",
    "\n",
    "    In Backpropagation, hidden layers and the feature detection matrix bot gets modified.\n",
    "\n",
    "    Detect Dog\n",
    "\n",
    "    <img src=\"./Detect_dog.png\"/>\n",
    "\n",
    "    Detect Cat\n",
    "\n",
    "    <img src=\"./Detect_cat.png\"/>\n",
    "\n",
    "    Detection Examples\n",
    "\n",
    "    <img src=\"./Detection_examples.png\"/>\n",
    "\n",
    "### Complete CNN Process\n",
    "\n",
    "<img src=\"./Complete_CNN.png\"/>\n",
    "\n",
    "### Softmax\n",
    "Output of neurons is basically a real number, now to convert them to probability, **Softmax function** is applied.\n",
    "\n",
    "$$  f_{j}(z) = \\frac{e^{z_{j}}}{\\sum_{k}^{} e^{z_{k}}} $$\n",
    "\n",
    "### Cross Entropy\n",
    "\n",
    "Cost Function are used to evaluate the Neural Network Accuracy/Error, in ANN we used Mean Squared Error Method to calculate the cost\n",
    "$$ C = \\frac{1}{2} (\\hat{y}-y)^{2} $$\n",
    "\n",
    "Cross Entropy is also a cost function and it is better in case of classification, for regression MSE is better.\n",
    "\n",
    "Function for Cross Entropy\n",
    "$$ L_{i} = -\\log\\left ( \\frac{e^{f_{y_{i}}}}{\\sum_{j}^{} e^{f_{j}}} \\right ) $$\n",
    "\n",
    "Simplified Cross Entropy Function\n",
    "$$ H(p,q) = \\sum_{x}^{} p(x) \\log q(x) $$\n",
    "\n",
    "Example of Applying Cross Enropy\n",
    "\n",
    "<img src=\"./Cross_entropy_example.png\"/>\n",
    "\n",
    "Let's suppose two neural networks NN1 and NN2, below is the predictions of cat dog detection using NN1 and NN2\n",
    "\n",
    "<img src=\"./NN1_NN2.png\"/>\n",
    "\n",
    "Now let's compare them\n",
    "\n",
    "<img src=\"./NN1_vs_NN2.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing [ Image Augmentation ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the Training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True\n",
    ")\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    './Datasets/dataset/training_set/',\n",
    "    target_size = (64, 64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'\n",
    ")\n",
    "\n",
    "# Preprocessing the Test set\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_set = train_datagen.flow_from_directory(\n",
    "    './Datasets/dataset/test_set/',\n",
    "    target_size = (64, 64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "# Step-1 Convolution\n",
    "cnn.add(\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters = 32,\n",
    "        kernel_size = 3,\n",
    "        activation = 'relu',\n",
    "        input_shape = [64,64,3]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Step-2 Pooling\n",
    "cnn.add(\n",
    "    tf.keras.layers.MaxPool2D(\n",
    "        pool_size = 2,\n",
    "        strides = 2\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Step-3 Adding one more Convolution Layer and Pooling\n",
    "cnn.add(\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters = 32,\n",
    "        kernel_size = 3,\n",
    "        activation = 'relu'\n",
    "        )\n",
    "    )\n",
    "\n",
    "cnn.add(\n",
    "    tf.keras.layers.MaxPool2D(\n",
    "        pool_size = 2,\n",
    "        strides = 2\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Step-4 Flattening\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Step-5 Full Connection\n",
    "cnn.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units = 128,\n",
    "        activation = 'relu',\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step-6 Output Layer\n",
    "cnn.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units = 1,\n",
    "        activation = 'sigmoid',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 245s 964ms/step - loss: 0.6702 - accuracy: 0.5849 - val_loss: 0.6560 - val_accuracy: 0.5825\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 44s 177ms/step - loss: 0.6020 - accuracy: 0.6747 - val_loss: 0.6195 - val_accuracy: 0.6600\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 0.5520 - accuracy: 0.7210 - val_loss: 0.6128 - val_accuracy: 0.6510\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 44s 178ms/step - loss: 0.5136 - accuracy: 0.7467 - val_loss: 0.5391 - val_accuracy: 0.7335\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 44s 176ms/step - loss: 0.5001 - accuracy: 0.7519 - val_loss: 0.5062 - val_accuracy: 0.7595\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 44s 178ms/step - loss: 0.4792 - accuracy: 0.7676 - val_loss: 0.4836 - val_accuracy: 0.7585\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 44s 177ms/step - loss: 0.4567 - accuracy: 0.7832 - val_loss: 0.4937 - val_accuracy: 0.7620\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 46s 185ms/step - loss: 0.4445 - accuracy: 0.7940 - val_loss: 0.4736 - val_accuracy: 0.7745\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.4346 - accuracy: 0.7951 - val_loss: 0.4662 - val_accuracy: 0.7840\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 44s 177ms/step - loss: 0.4170 - accuracy: 0.8035 - val_loss: 0.4611 - val_accuracy: 0.7905\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 44s 177ms/step - loss: 0.4066 - accuracy: 0.8135 - val_loss: 0.4464 - val_accuracy: 0.7925\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.3898 - accuracy: 0.8195 - val_loss: 0.4377 - val_accuracy: 0.8035\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 45s 182ms/step - loss: 0.3789 - accuracy: 0.8319 - val_loss: 0.4568 - val_accuracy: 0.7805\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.3728 - accuracy: 0.8282 - val_loss: 0.4800 - val_accuracy: 0.7835\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.3598 - accuracy: 0.8409 - val_loss: 0.4497 - val_accuracy: 0.7925\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.3491 - accuracy: 0.8460 - val_loss: 0.4395 - val_accuracy: 0.8140\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.3353 - accuracy: 0.8520 - val_loss: 0.4583 - val_accuracy: 0.8070\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.3208 - accuracy: 0.8612 - val_loss: 0.4970 - val_accuracy: 0.7880\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.3115 - accuracy: 0.8634 - val_loss: 0.4994 - val_accuracy: 0.7915\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.3025 - accuracy: 0.8671 - val_loss: 0.4572 - val_accuracy: 0.8090\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.2878 - accuracy: 0.8751 - val_loss: 0.4872 - val_accuracy: 0.7905\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 43s 172ms/step - loss: 0.2784 - accuracy: 0.8800 - val_loss: 0.4853 - val_accuracy: 0.8005\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.2712 - accuracy: 0.8829 - val_loss: 0.4976 - val_accuracy: 0.7995\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 43s 170ms/step - loss: 0.2592 - accuracy: 0.8925 - val_loss: 0.4723 - val_accuracy: 0.8040\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.2507 - accuracy: 0.8890 - val_loss: 0.4840 - val_accuracy: 0.7995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e6ca851a80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the CNN\n",
    "cnn.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "# Training the CNN on Training set and Evaluating on Test set\n",
    "cnn.fit(\n",
    "    x = training_set,\n",
    "    validation_data = test_set,\n",
    "    epochs = 25\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n",
      "Input Image contains a Dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import load_img, img_to_array \n",
    "\n",
    "test_image = load_img(\n",
    "    'Datasets/dataset/single_prediction/cat_or_dog_1.jpg',\n",
    "    target_size = (64, 64)\n",
    ")\n",
    "\n",
    "test_image = img_to_array(test_image)\n",
    "\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "\n",
    "training_set.class_indices \n",
    "\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'Dog'\n",
    "else:\n",
    "    prediction = 'Cat'\n",
    "    \n",
    "print(\"Input Image contains a\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making one more prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "Input Image contains a Cat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import load_img, img_to_array \n",
    "\n",
    "test_image = load_img(\n",
    "    'Datasets/dataset/single_prediction/cat_or_dog_2.jpg',\n",
    "    target_size = (64, 64)\n",
    ")\n",
    "\n",
    "test_image = img_to_array(test_image)\n",
    "\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "result = cnn.predict(test_image)\n",
    "\n",
    "training_set.class_indices \n",
    "\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'Dog'\n",
    "else:\n",
    "    prediction = 'Cat'\n",
    "    \n",
    "print(\"Input Image contains a\", prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('mlvenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "321334cedfa16b8be24125e5ab0e3623912f69d4b899407428e8bc07c2d3a5b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
